{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "\n",
        "Ans:K-Nearest Neighbors (KNN) is one of the most intuitive \"lazy learning\" algorithms in machine learning. Its core philosophy is simple: similar things exist in close proximity.\n",
        "\n",
        "\n",
        "Instead of building a complex mathematical model, KNN remembers the entire training dataset and makes predictions based on how \"close\" a new data point is to the existing points.\n",
        "\n",
        "\n",
        "\n",
        "How KNN Works: The Logic\n",
        "\n",
        "\n",
        "At its heart, the algorithm follows a four-step process:\n",
        "\n",
        "\n",
        " 1. Choose the number of $k$: Decide how many neighbors to look at (e.g., $k=3$ or $k=5$).\n",
        " 2. Calculate Distance: When a new data point arrives, the algorithm calculates the distance between that point and every other point in the dataset.\n",
        " - Most commonly, it uses Euclidean Distance: $d(p, q) = \\sqrt{\\sum_{i=1}^{n} (q_i - p_i)^2}$\n",
        " 3. Find Neighbors: It identifies the $k$ points that are closest to the new data point.\n",
        " 4. Vote or Average: It looks at those neighbors to determine the output.\n",
        "\n",
        "\n",
        "\n",
        "1. KNN for Classification\n",
        "\n",
        "In classification, the goal is to assign the new data point to a discrete category (e.g., \"Apple\" vs. \"Orange\" or \"Fraud\" vs. \"Legitimate\").\n",
        "\n",
        "\n",
        " - The Mechanism: The algorithm takes a \"majority vote\" from the $k$ nearest neighbors.\n",
        " - The Result: The new data point is assigned to the class that is most common among its neighbors.\n",
        " - Example: If $k=5$ and three neighbors are \"Blue\" while two are \"Red,\" the new point is classified as \"Blue.\"\n",
        "\n",
        "\n",
        "2. KNN for Regression\n",
        "\n",
        "\n",
        "In regression, the goal is to predict a continuous numerical value (e.g., the price of a house or the temperature tomorrow).\n",
        "\n",
        "\n",
        "\n",
        " - The Mechanism: Instead of voting, the algorithm takes the average (or mean) of the values of the $k$ nearest neighbors.\n",
        " - The Result: The predicted value is the mean of the neighbors' target values.\n",
        " - Example: If you are predicting house prices with $k=3$ and the three closest houses cost $300k, $310k, and $320k, KNN will predict the new house costs $310k."
      ],
      "metadata": {
        "id": "yitfhRDh6VNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Ans:The Curse of Dimensionality refers to the phenomenon where data becomes increasingly sparse as the number of features (dimensions) grows. In high-dimensional spaces, the volume of the space increases so rapidly that the available data points cannot maintain density, making them appear isolated and equidistant from one another.\n",
        "\n",
        "\n",
        "\n",
        "How it affects KNN performance:\n",
        "\n",
        " - Distance Meaninglessness: KNN relies on the assumption that \"nearness\" implies similarity. In high dimensions, the difference between the distance to the nearest neighbor and the distance to the farthest neighbor tends toward zero. When all points are roughly the same distance away, the concept of a \"neighbor\" loses its discriminative power.\n",
        " - Data Sparsity: To maintain the same level of statistical significance as you add dimensions, the amount of data required grows exponentially. Without an astronomical increase in sample size, the \"nearest\" neighbors found by the algorithm are likely too far away in the feature space to be truly similar.\n",
        " - Noise Sensitivity: KNN treats every dimension as equally important in its distance calculation (typically Euclidean distance). If many of the high-dimensional features are irrelevant or \"noise,\" they dominate the distance metric, drowning out the signal from the relevant features.\n",
        " - Computational Latency: The time complexity of KNN increases linearly with the number of dimensions ($O(D)$). As dimensions scale into the thousands, calculating the distance between the query point and every training point becomes computationally expensive and slow."
      ],
      "metadata": {
        "id": "-X-RhV_m9r_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Ans:Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a large set of variables into a smaller one while retaining as much variance (information) as possible.\n",
        "\n",
        "\n",
        "It achieves this by creating new, uncorrelated variables called Principal Components. These components are linear combinations of the original features, ordered by how much of the dataset's total \"spread\" or information they capture.\n",
        "\n",
        "The Difference: PCA vs. Feature Selection\n",
        "\n",
        "The core difference lies in whether you are keeping original variables or transforming them into something new.\n",
        "\n",
        "\n",
        "1. Feature Selection (Filtering)\n",
        "\n",
        "\n",
        " - Action: You select a subset of the original variables and discard the rest (e.g., keeping \"Age\" and \"Income\" but dropping \"Zip Code\").\n",
        " - Integrity: The physical meaning of the data remains the same.\n",
        " - Goal: To identify the most important existing variables.\n",
        "\n",
        "\n",
        "2. PCA (Feature Extraction)\n",
        "\n",
        " - Action: You combine all your original variables to create entirely new ones.\n",
        " - Integrity: The original variables are \"lost\" in the transformation. A Principal Component might be a mix of 30% Age, 50% Income, and 20% Zip Code.\n",
        " - Goal: To compress the data into a more efficient format by finding underlying patterns."
      ],
      "metadata": {
        "id": "Oq-nUlcj-cQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "\n",
        "Ans:In Principal Component Analysis (PCA), eigenvectors and eigenvalues are the mathematical tools used to decompose a dataset into its most informative parts.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. What they are\n",
        "\n",
        "\n",
        "When you analyze a dataset, you look at the covariance matrix, which shows how variables relate to one another.\n",
        "\n",
        "\n",
        " - Eigenvectors: These are vectors that define the direction of the new axes (Principal Components). They point toward the areas of the data with the highest spread.\n",
        " - Eigenvalues: These are scalars that represent the magnitude or \"strength\" of the variance in the direction of their corresponding eigenvector.\n",
        "\n",
        "\n",
        "2. Why they are important\n",
        "\n",
        " - Identifying Variance: Eigenvalues tell you exactly how much information (variance) is captured by each axis. This allows you to quantify the importance of different features.\n",
        " - Dimensionality Reduction: By ranking eigenvalues from highest to lowest, you can discard the eigenvectors with small eigenvalues. This reduces the size of the data while retaining the most critical patterns.\n",
        " - Removing Redundancy: Eigenvectors are mathematically orthogonal (at 90-degree angles). This ensures that each new \"component\" is independent, effectively removing the correlation between your original variables.\n",
        " - Simplifying Complexity: They transform a complex, multi-dimensional \"cloud\" of data into a structured set of coordinates that are easier for machine learning models to process."
      ],
      "metadata": {
        "id": "e0ccqlDK_eGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "\n",
        "Ans:In a machine learning pipeline, PCA and KNN complement each other by balancing computational efficiency with predictive accuracy. Their relationship is primarily defined by how PCA prepares the data environment to suit KNN's mathematical requirements.\n",
        "\n",
        "\n",
        "1. Overcoming the Curse of Dimensionality\n",
        "\n",
        "KNN relies on distance metrics (like Euclidean distance). In high-dimensional spaces, data points become sparse, and the distance between the nearest and farthest neighbors converges, making \"closeness\" meaningless.\n",
        "\n",
        " - The Complement: PCA reduces the number of variables, \"densifying\" the space so that KNN can find meaningful clusters and neighbors.\n",
        "\n",
        "\n",
        "2. Improving Computational Speed\n",
        "\n",
        "\n",
        "KNN is a \"lazy learner\" that calculates distances across all features for every prediction. As the number of features ($n$) increases, the search time grows significantly.\n",
        "\n",
        "\n",
        " - The Complement: By transforming $100$ features into $10$ principal components, PCA reduces the mathematical operations required for every KNN query by an order of magnitude.\n",
        "\n",
        "\n",
        "\n",
        "3. Noise Filtering\n",
        "\n",
        "Raw datasets often contain redundant or highly correlated features that can \"confuse\" KNN, as it treats all dimensions with equal importance.\n",
        "\n",
        "\n",
        " - The Complement: PCA isolates the principal components that capture the most variance (the signal) and discards the components that represent random fluctuations (the noise). This leads to a more robust KNN model.\n",
        "\n",
        "\n",
        "\n",
        "4. Resolving Multicollinearity\n",
        "\n",
        "\n",
        "KNN can be biased if multiple features are highly correlated, as it effectively \"double-counts\" that specific information when calculating distance.\n",
        "\n",
        "\n",
        " - The Complement: PCA transforms correlated features into a set of linearly uncorrelated (orthogonal) components, ensuring each dimension KNN looks at provides unique information."
      ],
      "metadata": {
        "id": "JQfEg7JzBUaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "\n",
        "\n",
        "Ans:Training a K-Nearest Neighbors (KNN) classifier on the Wine dataset is a classic way to demonstrate why feature scaling is so important.\n",
        "\n",
        "\n",
        "Since KNN relies on the Euclidean distance between points to make predictions, features with larger numerical ranges (like \"Magnesium\" which ranges from 70 to 160) will disproportionately dominate the distance calculation over features with smaller ranges (like \"Total Phenols\" which ranges from 0.9 to 3.8)."
      ],
      "metadata": {
        "id": "wzNmyw1MC477"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps2SFALz6NU4",
        "outputId": "939e68e3-7f1f-4b9b-f3fd-7c0d31ea017d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling: 0.7407\n",
            "Accuracy WITH Scaling:    0.9630\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- CASE 1: Without Feature Scaling ---\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# --- CASE 2: With Feature Scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# --- Comparison Output ---\n",
        "print(f\"Accuracy WITHOUT Scaling: {acc_unscaled:.4f}\")\n",
        "print(f\"Accuracy WITH Scaling:    {acc_scaled:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        "\n",
        "\n",
        "Ans:To perform Principal Component Analysis (PCA) on the Wine dataset, we first need to standardize the features. PCA is sensitive to the scale of the data because it seeks to maximize variance; without scaling, a feature with a large numerical range would dominate the components regardless of its actual importance."
      ],
      "metadata": {
        "id": "3If04ysYDvnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# 2. Standardize the features (Mean=0, Variance=1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Initialize and fit PCA\n",
        "# We'll calculate all components to see the full variance distribution\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# 4. Print the explained variance ratio\n",
        "print(\"Explained Variance Ratio per Principal Component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")\n",
        "\n",
        "# Cumulative variance to show how much information is retained\n",
        "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
        "print(f\"\\nTotal variance explained by first 2 components: {cumulative_variance[1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-cT6ZxlDhBa",
        "outputId": "b226216a-7b38-4a51-a3f9-3e5f35c2f2cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio per Principal Component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n",
            "\n",
            "Total variance explained by first 2 components: 0.5541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n",
        "\n",
        "\n",
        "Ans:To compare a K-Nearest Neighbors (KNN) classifier on original data versus data reduced via Principal Component Analysis (PCA), we generally follow a pipeline of scaling, transforming, and then evaluating."
      ],
      "metadata": {
        "id": "J9IA0ZgXEbH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_name_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Dataset (Iris is perfect for this comparison)\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split and Scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# 3. KNN on Original Data (4 features)\n",
        "knn_orig = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_orig.fit(X_train_std, y_train)\n",
        "y_pred_orig = knn_orig.predict(X_test_std)\n",
        "acc_orig = accuracy_score(y_test, y_pred_orig)\n",
        "\n",
        "# 4. PCA Transformation (Top 2 Components)\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_std)\n",
        "X_test_pca = pca.transform(X_test_std)\n",
        "\n",
        "# 5. KNN on PCA-Transformed Data (2 features)\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Output Results\n",
        "print(f\"Accuracy on Original Dataset (4 features): {acc_orig:.4f}\")\n",
        "print(f\"Accuracy on PCA-Transformed Dataset (2 components): {acc_pca:.4f}\")\n",
        "print(f\"Variance explained by top 2 components: {np.sum(pca.explained_variance_ratio_):.2%}\")"
      ],
      "metadata": {
        "id": "CaOhKk6fE-0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "\n",
        "Ans:To compare how different distance metrics affect a K-Nearest Neighbors (KNN) classifier, we’ll use the Wine dataset from sklearn. This dataset contains 13 features (chemical analyses) for three different cultivars of wine."
      ],
      "metadata": {
        "id": "ONB8w06WFJ9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load and split the dataset\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Compare Metrics\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "\n",
        "print(f\"{'Metric':<15} | {'Accuracy Score':<15}\")\n",
        "print(\"-\" * 33)\n",
        "\n",
        "for m in metrics:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, metric=m)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{m.capitalize():<15} | {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8FZjyiVFr5k",
        "outputId": "ddae8210-2356-45a2-b08a-6394f81a7c0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric          | Accuracy Score \n",
            "---------------------------------\n",
            "Euclidean       | 0.9444\n",
            "Manhattan       | 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "\n",
        "\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "\n",
        "Ans:Dealing with high-dimensional gene expression data (often called the \"large $p$, small $n$\" problem) is a classic challenge in bioinformatics. When you have thousands of genes but only dozens of patients, models tend to memorize the noise rather than the signal.\n",
        "\n",
        "\n",
        "1. Dimensionality Reduction with PCA\n",
        "\n",
        "\n",
        "Principal Component Analysis (PCA) transforms your massive list of correlated genes into a smaller set of uncorrelated variables called Principal Components (PCs).\n",
        "\n",
        "\n",
        " - Standardization: Before running PCA, we must scale the data (mean = 0, variance = 1). Since gene expression levels can vary by orders of magnitude, scaling ensures that high-abundance genes don’t dominate the analysis simply because of their scale.\n",
        " - Orthogonal Transformation: PCA identifies the direction (PC1) along which the data varies the most, then the second-most (PC2), and so on.\n",
        "\n",
        "\n",
        "2. Deciding on the Number of Components\n",
        "\n",
        "\n",
        "We don't want to keep all components, as that defeats the purpose. We use two main methods:\n",
        "\n",
        "\n",
        " - The Scree Plot: We look for the \"elbow\"—the point where the amount of additional variance explained by each new component drops off significantly.\n",
        " - Cumulative Variance Threshold: A common rule of thumb is to retain enough components to explain 70% to 90% of the total variance in the dataset.\n",
        "\n",
        "\n",
        "3. KNN Classification Post-Reduction\n",
        "\n",
        "\n",
        "Once the data is projected onto the top $k$ components, we apply the K-Nearest Neighbors (KNN) algorithm.\n",
        "\n",
        "\n",
        " - The Logic: In the reduced PCA space, patients with similar cancer types should cluster together. KNN classifies a new patient based on the majority label of their $k$ closest neighbors.\n",
        " - Distance Metric: We typically use Euclidean distance. Because PCA has already decorrelated the features, this distance metric becomes much more meaningful and less susceptible to the \"curse of dimensionality.\"\n",
        "\n",
        "\n",
        "\n",
        "4. Model Evaluation\n",
        "\n",
        "Given the clinical sensitivity of cancer diagnosis, we cannot rely on simple accuracy alone.\n",
        "\n",
        "\n",
        " - Stratified Cross-Validation: We use $k$-fold cross-validation, ensuring each fold has a representative proportion of each cancer type.\n",
        " - Metrics: * Precision/Recall: Crucial for understanding false positives vs. false negatives\n",
        " - F1-Score: The harmonic mean of precision and recall.\n",
        " - Confusion Matrix: To see which specific cancer types are being confused with one another.\n",
        "\n",
        "\n",
        "5. Justification for Stakeholders\n",
        "\n",
        "\n",
        "To a non-technical stakeholder, this pipeline offers three major benefits:\n",
        "\n",
        "\n",
        " 1. Noise Filtration: By using PCA, we strip away the \"background noise\" of the genome and focus only on the strongest biological signals driving the disease.\n",
        " 2. Efficiency: KNN is computationally expensive on large datasets, but by reducing the data first, the model becomes fast enough for real-time clinical decision support.\n",
        " 3. Stability: This approach prevents \"overfitting,\" meaning the model isn't just good at identifying the patients we've already seen—it’s actually learning the underlying patterns of the cancer, making it much more reliable for new, unseen patients."
      ],
      "metadata": {
        "id": "9j5PGG3_Fw2L"
      }
    }
  ]
}